import os
import cv2
import json
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import time
from sklearn.model_selection import train_test_split


# ---------------------- 1. æ•°æ®å‡†å¤‡æ¨¡å—ï¼ˆæ–°å¢ï¼šå¤„ç†è®­ç»ƒæ•°æ®ï¼‰ ----------------------
class DocDataset(Dataset):
    """æ–‡æ¡£æ•°æ®é›†ç±»ï¼Œç”¨äºåŠ è½½å’Œé¢„å¤„ç†è®­ç»ƒå›¾ç‰‡"""

    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths  # å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        self.labels = labels  # æ ‡ç­¾åˆ—è¡¨ï¼ˆ0:å‘ç¥¨, 1:èµ„äº§è´Ÿå€ºè¡¨, 2:åˆ©æ¶¦è¡¨, 3:è¥ä¸šæ‰§ç…§ï¼‰
        self.transform = transform  # é¢„å¤„ç†æ–¹æ³•

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        # è¯»å–å›¾ç‰‡å¹¶è½¬ä¸ºç°åº¦å›¾
        img_path = self.image_paths[idx]
        img = Image.open(img_path).convert('L')  # è½¬ä¸ºç°åº¦å›¾
        label = self.labels[idx]

        # åº”ç”¨é¢„å¤„ç†
        if self.transform:
            img = self.transform(img)
        return img, label


def prepare_train_data(data_dir):
    """
    å‡†å¤‡è®­ç»ƒæ•°æ®
    :param data_dir: æ•°æ®æ–‡ä»¶å¤¹ï¼Œç»“æ„åº”ä¸ºï¼š
                     data_dir/
                        0_å‘ç¥¨/
                        1_èµ„äº§è´Ÿå€ºè¡¨/
                        2_åˆ©æ¶¦è¡¨/
                        3_è¥ä¸šæ‰§ç…§/
    """
    image_paths = []
    labels = []

    # éå†æ¯ä¸ªç±»åˆ«æ–‡ä»¶å¤¹
    for label in [0, 1, 2, 3]:
        class_dir = os.path.join(data_dir, f"{label}_{['å‘ç¥¨', 'èµ„äº§è´Ÿå€ºè¡¨', 'åˆ©æ¶¦è¡¨', 'è¥ä¸šæ‰§ç…§'][label]}")
        if not os.path.exists(class_dir):
            raise FileNotFoundError(f"ç±»åˆ«æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼š{class_dir}\nè¯·æŒ‰è¦æ±‚åˆ›å»ºæ•°æ®æ–‡ä»¶å¤¹ç»“æ„")

        # æ”¶é›†è¯¥ç±»åˆ«ä¸‹çš„æ‰€æœ‰å›¾ç‰‡
        for img_name in os.listdir(class_dir):
            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                image_paths.append(os.path.join(class_dir, img_name))
                labels.append(label)

    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆ8:2ï¼‰
    train_paths, val_paths, train_labels, val_labels = train_test_split(
        image_paths, labels, test_size=0.2, random_state=42
    )

    # å®šä¹‰è®­ç»ƒé›†é¢„å¤„ç†ï¼ˆå¢åŠ æ•°æ®å¢å¼ºï¼‰
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomRotation(10),  # éšæœºæ—‹è½¬ï¼Œå¢å¼ºæ³›åŒ–èƒ½åŠ›
        transforms.ColorJitter(contrast=2.0),  # å¢å¼ºå¯¹æ¯”åº¦ï¼Œçªå‡ºæ–‡å­—
        transforms.ToTensor(),
        transforms.Normalize(mean=[127.5], std=[127.5])
    ])

    # éªŒè¯é›†å’Œæ¨ç†ç”¨ç›¸åŒé¢„å¤„ç†
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[127.5], std=[127.5])
    ])

    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = DocDataset(train_paths, train_labels, train_transform)
    val_dataset = DocDataset(val_paths, val_labels, val_transform)

    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)

    print(f"ğŸ“Š æ•°æ®å‡†å¤‡å®Œæˆ | è®­ç»ƒæ ·æœ¬ï¼š{len(train_dataset)} | éªŒè¯æ ·æœ¬ï¼š{len(val_dataset)}")
    return train_loader, val_loader


# ---------------------- 2. æ¨¡å‹å®šä¹‰ï¼ˆä¼˜åŒ–ç‰ˆï¼šå¢å¼ºç‰¹å¾æå–ï¼‰ ----------------------
class DeepSeekDocTypeModel(nn.Module):
    """ä¼˜åŒ–ç‰ˆæ–‡æ¡£åˆ†ç±»æ¨¡å‹ï¼Œå¢å¼ºå¯¹å‘ç¥¨å’Œåˆ©æ¶¦è¡¨çš„åŒºåˆ†èƒ½åŠ›"""

    def __init__(self, num_classes: int = 4):
        super().__init__()
        # å¢å¼ºç‰¹å¾æå–å±‚ï¼šæ›´å…³æ³¨æ–‡å­—å’Œè¡¨æ ¼ç‰¹å¾
        self.features = nn.Sequential(
            # ç¬¬ä¸€å±‚ï¼šæ•æ‰åŸºç¡€è¾¹ç¼˜ç‰¹å¾
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2),

            # ç¬¬äºŒå±‚ï¼šæ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼Œèšç„¦å±€éƒ¨ç‰¹å¾ï¼ˆå¦‚å‘ç¥¨ç« ã€è¡¨æ ¼çº¿ï¼‰
            nn.Conv2d(32, 64, kernel_size=3, padding=1, groups=32),
            nn.Conv2d(64, 64, kernel_size=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),

            # ç¬¬ä¸‰å±‚ï¼šå¢å¼ºå¯¹ç»†èŠ‚ç‰¹å¾çš„æ•æ‰ï¼ˆå¦‚"å‘ç¥¨"vs"åˆ©æ¶¦è¡¨"æ–‡å­—ï¼‰
            nn.Conv2d(64, 128, kernel_size=3, padding=1, groups=64),
            nn.Conv2d(128, 128, kernel_size=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2),

            # æ–°å¢ç¬¬å››å±‚ï¼šå¼ºåŒ–å¯¹å¤æ‚ç‰¹å¾çš„å­¦ä¹ 
            nn.Conv2d(128, 256, kernel_size=3, padding=1, groups=128),
            nn.Conv2d(256, 256, kernel_size=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )

        # åˆ†ç±»å±‚ï¼šæ›´å¤æ‚çš„å…¨è¿æ¥ç½‘ç»œ
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),  # é˜²æ­¢è¿‡æ‹Ÿåˆ
            nn.Linear(128, num_classes)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.features(x)
        x = self.classifier(x)
        return x


# ---------------------- 3. è®­ç»ƒæ¨¡å—ï¼ˆæ–°å¢ï¼šæ¨¡å‹è®­ç»ƒåŠŸèƒ½ï¼‰ ----------------------
def train_model(data_dir, epochs=20):
    """è®­ç»ƒæ¨¡å‹å¹¶ä¿å­˜æƒé‡"""
    # å‡†å¤‡æ•°æ®
    train_loader, val_loader = prepare_train_data(data_dir)

    # åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = DeepSeekDocTypeModel().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()  # åˆ†ç±»é—®é¢˜å¸¸ç”¨æŸå¤±å‡½æ•°

    best_val_acc = 0.0  # è®°å½•æœ€ä½³éªŒè¯å‡†ç¡®ç‡

    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼
        train_loss = 0.0
        train_correct = 0
        total_train = 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            # å‰å‘ä¼ æ’­
            outputs = model(images)
            loss = criterion(outputs, labels)

            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
            loss.backward()  # è®¡ç®—æ¢¯åº¦
            optimizer.step()  # æ›´æ–°å‚æ•°

            # ç»Ÿè®¡è®­ç»ƒæŒ‡æ ‡
            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_train += labels.size(0)
            train_correct += (predicted == labels).sum().item()

        # éªŒè¯é˜¶æ®µ
        model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        val_loss = 0.0
        val_correct = 0
        total_val = 0

        with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)

                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total_val += labels.size(0)
                val_correct += (predicted == labels).sum().item()

        # è®¡ç®—å‡†ç¡®ç‡
        train_acc = 100 * train_correct / total_train
        val_acc = 100 * val_correct / total_val

        # æ‰“å°æœ¬è½®ç»“æœ
        print(f"\nEpoch {epoch + 1}/{epochs}")
        print(f"è®­ç»ƒæŸå¤±: {train_loss / len(train_loader):.4f} | è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}%")
        print(f"éªŒè¯æŸå¤±: {val_loss / len(val_loader):.4f} | éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), "deepseek_doc_type_model.pth")
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆéªŒè¯å‡†ç¡®ç‡ï¼š{best_val_acc:.2f}%ï¼‰")

    print(f"\nğŸ è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡ï¼š{best_val_acc:.2f}% | æ¨¡å‹å·²ä¿å­˜ä¸º deepseek_doc_type_model.pth")
    return model


# ---------------------- 4. æ¨ç†æ¨¡å—ï¼ˆä¼˜åŒ–ç‰ˆï¼šå¢åŠ æ¦‚ç‡è¿‡æ»¤ï¼‰ ----------------------
def deepseek_doc_type_recognition(
        img_path: str = r"C:\Users\thy\Desktop\å›¾åƒ1.png",
        output_json: str = r"C:\Users\thy\Desktop\è´·æ¬¾å®¢æˆ·æ–‡æ¡£è¯†åˆ«ç»“æœ.json",
        doc_type_mapping: dict = {
            0: "å‘ç¥¨",
            1: "èµ„äº§è´Ÿå€ºè¡¨",
            2: "åˆ©æ¶¦è¡¨",
            3: "è¥ä¸šæ‰§ç…§"
        }
) -> str:
    """æ–‡æ¡£ç±»å‹è¯†åˆ«ä¸»å‡½æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼šå¢åŠ ä½æ¦‚ç‡è¿‡æ»¤ï¼‰"""
    # 1. ç¯å¢ƒåˆå§‹åŒ–
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”§ æ¨ç†ç¯å¢ƒï¼š{device} | æ”¯æŒç±»å‹ï¼š{list(doc_type_mapping.values())}")

    # 2. å›¾åƒé¢„å¤„ç†ï¼ˆä¸éªŒè¯é›†ä¿æŒä¸€è‡´ï¼‰
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[127.5], std=[127.5])
    ])

    try:
        # è¯»å–å›¾åƒ
        img = Image.open(img_path).convert('L')  # ç”¨PILè¯»å–ç°åº¦å›¾
        img_np = np.array(img)  # ç”¨äºè®°å½•å°ºå¯¸ä¿¡æ¯
        img_tensor = transform(img).unsqueeze(0).to(device)
        print(f"ğŸ“¥ åŠ è½½æˆåŠŸï¼š{os.path.basename(img_path)} | å°ºå¯¸ï¼š{img_np.shape[0]}Ã—{img_np.shape[1]}")
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥ï¼š{str(e)}")
        return json.dumps({"è¯†åˆ«çŠ¶æ€": "å¤±è´¥", "é”™è¯¯ä¿¡æ¯": str(e)}, ensure_ascii=False, indent=4)

    # 3. æ¨¡å‹åŠ è½½
    model = DeepSeekDocTypeModel().to(device)
    try:
        model.load_state_dict(torch.load("deepseek_doc_type_model.pth", map_location=device))
        print("âœ… åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹æˆåŠŸ")
    except FileNotFoundError:
        print("âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨éšæœºæ¨¡å‹ï¼ˆæ•ˆæœå·®ï¼è¯·å…ˆè®­ç»ƒï¼‰")

    # 4. æ¨ç†è®¡ç®—ï¼ˆå¢åŠ æ¦‚ç‡è¿‡æ»¤ï¼‰
    model.eval()
    start_time = time.time()
    with torch.no_grad():
        logits = model(img_tensor)
        pred_probs = F.softmax(logits, dim=1).cpu().numpy()[0]  # æ¦‚ç‡å½’ä¸€åŒ–
        pred_type_idx = int(torch.argmax(logits, dim=1).item())
        pred_doc_type = doc_type_mapping[pred_type_idx]

        # ä½æ¦‚ç‡è¿‡æ»¤ï¼ˆä½äº60%æ ‡è®°ä¸ºä¸ç¡®å®šï¼‰
        if max(pred_probs) < 0.6:
            pred_doc_type = f"æ— æ³•ç¡®å®šï¼ˆç–‘ä¼¼{pred_doc_type}ï¼Œå»ºè®®äººå·¥å¤æ ¸ï¼‰"

        infer_time = time.time() - start_time

    # 5. ç»“æœæ•´ç†
    final_result = {
        "è¯†åˆ«çŠ¶æ€": "æˆåŠŸ",
        "æ–‡ä»¶ä¿¡æ¯": {
            "æ–‡ä»¶å": os.path.basename(img_path),
            "è·¯å¾„": img_path,
            "å¤§å°(KB)": round(os.path.getsize(img_path) / 1024, 2),
            "å°ºå¯¸": f"{img_np.shape[0]}Ã—{img_np.shape[1]}"
        },
        "è¯†åˆ«ç»“æœ": {
            "æ–‡æ¡£ç±»å‹": pred_doc_type,
            "å„ç±»åˆ«æ¦‚ç‡": {doc_type_mapping[i]: round(float(pred_probs[i]), 4) for i in range(4)},
            "æ¨ç†æ—¶é—´(ç§’)": round(infer_time, 4),
            "è®¾å¤‡": str(device)
        },
        "ç”Ÿæˆæ—¶é—´": time.strftime("%Y-%m-%d %H:%M:%S")
    }

    # 6. ä¿å­˜ç»“æœ
    try:
        with open(output_json, "w", encoding="utf-8") as f:
            json.dump(final_result, f, ensure_ascii=False, indent=4)
        print(f"\nğŸ‰ ç»“æœå·²ä¿å­˜è‡³ï¼š{os.path.abspath(output_json)}")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜å¤±è´¥ï¼š{str(e)}")

    return json.dumps(final_result, ensure_ascii=False, indent=4)


# ---------------------- 5. æ‰§è¡Œå…¥å£ ----------------------
if __name__ == "__main__":
    # é¦–æ¬¡è¿è¡Œè¯·å®‰è£…ä¾èµ–
    print("âš ï¸ é¦–æ¬¡è¿è¡Œè¯·å®‰è£…ä¾èµ–ï¼špip install torch torchvision opencv-python numpy pillow scikit-learn")

    # æ­¥éª¤1ï¼šè®­ç»ƒæ¨¡å‹ï¼ˆè¯·å…ˆæŒ‰è¦æ±‚å‡†å¤‡æ•°æ®ï¼‰
    # æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆè¯·ä¿®æ”¹ä¸ºä½ çš„å®é™…è·¯å¾„ï¼‰
    data_directory = r"C:\Users\thy\Desktop\æ–‡æ¡£è®­ç»ƒæ•°æ®"  # é‡Œé¢åº”æœ‰4ä¸ªå­æ–‡ä»¶å¤¹ï¼ˆ0_å‘ç¥¨åˆ°3_è¥ä¸šæ‰§ç…§ï¼‰
    if not os.path.exists(data_directory):
        print("\nâš ï¸ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶å¤¹ï¼Œè¯·æŒ‰ä»¥ä¸‹ç»“æ„åˆ›å»ºï¼š")
        print(f"{data_directory}/")
        print("  â”œâ”€0_å‘ç¥¨/       ï¼ˆå­˜æ”¾å‘ç¥¨å›¾ç‰‡ï¼‰")
        print("  â”œâ”€1_èµ„äº§è´Ÿå€ºè¡¨/ ï¼ˆå­˜æ”¾èµ„äº§è´Ÿå€ºè¡¨å›¾ç‰‡ï¼‰")
        print("  â”œâ”€2_åˆ©æ¶¦è¡¨/     ï¼ˆå­˜æ”¾åˆ©æ¶¦è¡¨å›¾ç‰‡ï¼‰")
        print("  â””â”€3_è¥ä¸šæ‰§ç…§/   ï¼ˆå­˜æ”¾è¥ä¸šæ‰§ç…§å›¾ç‰‡ï¼‰")
    else:
        print("\nğŸ“Œ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        train_model(data_directory, epochs=20)  # è®­ç»ƒ20è½®ï¼ˆå¯è°ƒæ•´ï¼‰

    # æ­¥éª¤2ï¼šè¯†åˆ«æ–‡æ¡£ï¼ˆè®­ç»ƒå®Œæˆåæ‰§è¡Œï¼‰
    print("\nğŸ“Œ å¼€å§‹è¯†åˆ«æ–‡æ¡£...")
    result = deepseek_doc_type_recognition(
        img_path=r"C:\Users\thy\Desktop\å›¾åƒ1.png",  # å¾…è¯†åˆ«çš„å‘ç¥¨å›¾ç‰‡
        output_json=r"C:\Users\thy\Desktop\è¯†åˆ«ç»“æœ.json"
    )
    print("\nğŸ“Š è¯†åˆ«ç»“æœï¼š")
    print(result)
