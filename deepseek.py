import os
import cv2
import json
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import time


# æ•°æ®é›†ç±»ï¼ˆä¿®å¤è·¯å¾„å¤„ç†å’Œå›¾åƒè¯»å–é—®é¢˜ï¼‰
class SingleInvoiceDataset(Dataset):
    def __init__(self, single_image_path, transform=None):
        # æ ‡å‡†åŒ–è·¯å¾„
        self.single_image_path = os.path.normpath(single_image_path)
        self.image_paths = []

        print(f"ğŸ” æ£€æŸ¥å›¾åƒè·¯å¾„: {self.single_image_path}")
        print(f"ğŸ” æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {os.path.exists(self.single_image_path)}")

        # éªŒè¯å›¾åƒæ–‡ä»¶
        if not os.path.exists(self.single_image_path):
            raise FileNotFoundError(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨ï¼š{self.single_image_path}")
        elif not os.path.isfile(self.single_image_path):
            raise ValueError(f"âŒ ä¸æ˜¯æœ‰æ•ˆæ–‡ä»¶ï¼š{self.single_image_path}")

        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        valid_extensions = ('.jpg', '.png', '.jpeg', '.bmp', '.tiff', '.webp')
        file_ext = os.path.splitext(self.single_image_path)[1].lower()
        if file_ext not in valid_extensions:
            raise ValueError(f"âŒ ä¸æ”¯æŒçš„å›¾åƒæ ¼å¼ {file_ext}ï¼Œæ”¯æŒæ ¼å¼: {valid_extensions}")

        # éªŒè¯æ–‡ä»¶å¯è¯»å–
        try:
            # å°è¯•ç”¨OpenCVç›´æ¥è¯»å–éªŒè¯
            test_img = cv2.imread(self.single_image_path)
            if test_img is None:
                raise ValueError("OpenCVæ— æ³•è¯»å–è¯¥æ–‡ä»¶")

            self.image_paths.append(self.single_image_path)
            print(f"âœ… å›¾åƒéªŒè¯æˆåŠŸï¼š{os.path.basename(self.single_image_path)}")
            print(f"âœ… å›¾åƒå°ºå¯¸ï¼š{test_img.shape}")

        except Exception as e:
            raise ValueError(f"âŒ å›¾åƒæ–‡ä»¶æŸåæˆ–æ— æ³•è¯»å–ï¼š{str(e)}")

        self.transform = transform
        # ç®€åŒ–æ ‡ç­¾é€»è¾‘ï¼šå¦‚æœæ–‡ä»¶ååŒ…å«"æ­£å¸¸"åˆ™ä¸ºæ­£å¸¸å‘ç¥¨ï¼Œå¦åˆ™ä¸ºå¼‚å¸¸å‘ç¥¨
        filename = os.path.basename(self.single_image_path).lower()
        if "æ­£å¸¸" in filename:
            self.labels = [0]  # æ­£å¸¸å‘ç¥¨
            print(f"âœ… æ ¹æ®æ–‡ä»¶åæ ‡æ³¨ä¸ºï¼šæ­£å¸¸å‘ç¥¨")
        else:
            self.labels = [1]  # å¼‚å¸¸å‘ç¥¨
            print(f"âœ… æ ¹æ®æ–‡ä»¶åæ ‡æ³¨ä¸ºï¼šå¼‚å¸¸å‘ç¥¨")

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        img_name = os.path.basename(img_path)

        try:
            # ä½¿ç”¨OpenCVç›´æ¥è¯»å–å›¾åƒï¼ˆæ›´å¯é ï¼‰
            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # ç›´æ¥è¯»å–ä¸ºç°åº¦å›¾
            if image is None:
                raise ValueError("cv2.imreadè¿”å›Noneï¼Œæ–‡ä»¶å¯èƒ½æŸå")

            # å›¾åƒé¢„å¤„ç†
            image = cv2.resize(image, (224, 224))  # è°ƒæ•´å°ºå¯¸
            image = image.astype(np.float32) / 255.0  # å½’ä¸€åŒ–åˆ°[0,1]
            image = (image - 0.5) / 0.5  # æ ‡å‡†åŒ–åˆ°[-1,1]
            image = np.expand_dims(image, axis=0)  # (1, 224, 224)

            label = self.labels[idx]

            # è½¬æ¢ä¸ºtensor
            image_tensor = torch.from_numpy(image).float()

            return image_tensor, torch.tensor(label, dtype=torch.long), img_path

        except Exception as e:
            print(f"âŒ å›¾åƒå¤„ç†å¤±è´¥ {img_name}: {str(e)}")
            # è¿”å›ä¸€ä¸ªç©ºç™½å›¾åƒä½œä¸ºfallback
            blank_image = np.random.rand(1, 224, 224).astype(np.float32)
            blank_image = (blank_image - 0.5) / 0.5
            return torch.from_numpy(blank_image).float(), torch.tensor(0, dtype=torch.long), img_path


# æ¨¡æ‹ŸDeepSeekæ•™å¸ˆæ¨¡å‹ï¼ˆä¿®å¤é€»è¾‘ï¼‰
class MockDeepSeekTeacherModel:
    def __init__(self):
        print("â„¹ï¸ ä½¿ç”¨ã€æ¨¡æ‹ŸDeepSeekæ•™å¸ˆæ¨¡å‹ã€‘")
        self.noise = 0.05  # å‡å°‘å™ªå£°

    def get_teacher_logits(self, image_path):
        img_name = os.path.basename(image_path)
        print(f"â„¹ï¸ æ¨¡æ‹ŸDeepSeekæ¨ç†ï¼š{img_name}")

        # æ›´æ™ºèƒ½çš„æ¨¡æ‹Ÿé€»è¾‘
        filename_lower = img_name.lower()

        if "æ­£å¸¸" in filename_lower or "proper" in filename_lower or "good" in filename_lower:
            base_logits = torch.tensor([3.0, 0.5], dtype=torch.float32)  # å¼ºåå‘æ­£å¸¸
        elif "å¼‚å¸¸" in filename_lower or "abnormal" in filename_lower or "bad" in filename_lower:
            base_logits = torch.tensor([0.5, 3.0], dtype=torch.float32)  # å¼ºåå‘å¼‚å¸¸
        else:
            # ä¸­æ€§åˆ¤æ–­
            base_logits = torch.tensor([1.5, 1.5], dtype=torch.float32)

        # æ·»åŠ è½»å¾®éšæœºå™ªå£°
        random_noise = torch.randn_like(base_logits) * self.noise
        final_logits = base_logits + random_noise

        # é™åˆ¶logitsèŒƒå›´
        final_logits = torch.clamp(final_logits, min=0.1, max=4.0)

        pred_class = "æ­£å¸¸å‘ç¥¨" if final_logits[0] > final_logits[1] else "å¼‚å¸¸å‘ç¥¨"
        print(f"â„¹ï¸ æ¨¡æ‹ŸDeepSeeké¢„æµ‹ï¼š{pred_class}ï¼ˆlogits: {final_logits.tolist()}ï¼‰")

        return final_logits.unsqueeze(0)


# ç®€åŒ–å­¦ç”Ÿæ¨¡å‹ï¼ˆå‡å°‘å¤æ‚åº¦ï¼‰
class LightweightStudentModel(nn.Module):
    def __init__(self, num_classes=2):
        super().__init__()
        # æ›´ç®€å•çš„ç½‘ç»œç»“æ„
        self.conv_layers = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(32, num_classes)
        )

    def forward(self, x):
        x = self.conv_layers(x)
        x = self.classifier(x)
        return x


# çŸ¥è¯†è’¸é¦æ¨¡å—ï¼ˆä¿®å¤è®¾å¤‡å…¼å®¹æ€§ï¼‰
class KnowledgeDistillationModule(nn.Module):
    def __init__(self, teacher_model, student_model, kd_loss_coef=1.0):
        super().__init__()
        self.teacher = teacher_model
        self.student = student_model
        self.kd_loss_coef = kd_loss_coef
        self.ce_loss = nn.CrossEntropyLoss()
        self.temperature = 3.0  # é™ä½æ¸©åº¦å‚æ•°

    def forward(self, x, labels, img_path):
        student_logits = self.student(x)

        # è·å–æ•™å¸ˆlogitsï¼ˆä¸ä¼ å…¥è®¾å¤‡ï¼Œåœ¨å¤–éƒ¨å¤„ç†ï¼‰
        teacher_logits = self.teacher.get_teacher_logits(img_path[0] if isinstance(img_path, list) else img_path)

        # ç¡®ä¿teacher_logitsåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if hasattr(x, 'device'):
            teacher_logits = teacher_logits.to(x.device)

        # ç¡¬æ ‡ç­¾æŸå¤±
        hard_loss = self.ce_loss(student_logits, labels)

        # çŸ¥è¯†è’¸é¦æŸå¤±
        student_soft = F.log_softmax(student_logits / self.temperature, dim=1)
        teacher_soft = F.softmax(teacher_logits / self.temperature, dim=1)
        kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (self.temperature ** 2)

        total_loss = hard_loss + self.kd_loss_coef * kd_loss
        return total_loss, student_logits, teacher_logits


# ç®€åŒ–æŒ‡æ ‡è®¡ç®—ï¼ˆç§»é™¤torchmetricsä¾èµ–ï¼‰
def calculate_metrics(predictions, targets):
    """æ‰‹åŠ¨è®¡ç®—æŒ‡æ ‡"""
    if len(predictions) == 0:
        return 0.0, 0.0, 0.0, 0.0

    pred_np = np.array(predictions)
    target_np = np.array(targets)

    accuracy = np.mean(pred_np == target_np)

    # ç®€åŒ–è®¡ç®—ï¼šå¯¹äºäºŒåˆ†ç±»é—®é¢˜
    tp = np.sum((pred_np == 1) & (target_np == 1))
    fp = np.sum((pred_np == 1) & (target_np == 0))
    fn = np.sum((pred_np == 0) & (target_np == 1))

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0

    return accuracy, precision, recall, f1


# ä¸»å‡½æ•°ï¼ˆå…¨é¢ä¿®å¤ï¼‰
def invoice_recognition_single_image(single_image_path=None, output_json=None):
    print("===== å¼€å§‹å•å¼ å‘ç¥¨è¯†åˆ«æµç¨‹ï¼ˆæ¨¡æ‹ŸDeepSeekï¼‰ =====")

    # è®¾ç½®é»˜è®¤è·¯å¾„
    if single_image_path is None:
        single_image_path = "C:/Users/zzh/Desktop/wechat_2025-09-29_212608_948.png"
    if output_json is None:
        output_json = "C:/Users/zzh/Desktop/å•å¼ å‘ç¥¨è¯†åˆ«ç»“æœ.json"

    print(f"ç›®æ ‡å›¾åƒ: {single_image_path}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_json}")

    # è®¾å¤‡åˆå§‹åŒ–
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # 1. åŠ è½½å•å¼ å›¾åƒæ•°æ®é›†
    try:
        dataset = SingleInvoiceDataset(single_image_path)
        dataloader = DataLoader(dataset, batch_size=1, shuffle=False)
        print(f"âœ… æˆåŠŸåŠ è½½å›¾åƒï¼Œå¼€å§‹å¤„ç†...")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {str(e)}")
        return None

    # 2. åˆå§‹åŒ–æ¨¡å‹
    try:
        teacher_model = MockDeepSeekTeacherModel()
        student_model = LightweightStudentModel(num_classes=2).to(device)
        distillation_model = KnowledgeDistillationModule(teacher_model, student_model).to(device)
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return None

    # 3. æ¨ç†è¿‡ç¨‹
    distillation_model.eval()
    recognition_results = []

    # æ‰‹åŠ¨è®°å½•æŒ‡æ ‡
    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for batch_idx, (data, labels, img_paths) in enumerate(dataloader):
            img_path = img_paths[0] if isinstance(img_paths, (list, tuple)) else img_paths
            img_name = os.path.basename(img_path)
            print(f"\n----- å¤„ç†å›¾åƒ: {img_name} -----")

            try:
                data, labels = data.to(device), labels.to(device)
                print(f"âœ… æ•°æ®åŠ è½½åˆ°è®¾å¤‡: data.shape={data.shape}, label={labels.item()}")

                start_time = time.time()

                # æ ¸å¿ƒæ¨ç†
                total_loss, student_logits, teacher_logits = distillation_model(data, labels, img_path)
                infer_time = round(time.time() - start_time, 4)

                print(f"âœ… æ¨ç†å®Œæˆ: loss={total_loss.item():.4f}, time={infer_time}s")

                # è§£æè¾“å‡º
                student_probs = F.softmax(student_logits, dim=1).cpu().numpy()[0]
                pred_class_idx = torch.argmax(student_logits, dim=1).item()
                pred_class = "æ­£å¸¸å‘ç¥¨" if pred_class_idx == 0 else "å¼‚å¸¸å‘ç¥¨"
                true_class = "æ­£å¸¸å‘ç¥¨" if labels.item() == 0 else "å¼‚å¸¸å‘ç¥¨"

                # è®°å½•é¢„æµ‹ç»“æœ
                all_predictions.append(pred_class_idx)
                all_targets.append(labels.item())

                # ä¿å­˜ç»“æœ
                recognition_results.append({
                    "å›¾åƒæ–‡ä»¶å": img_name,
                    "å›¾åƒè·¯å¾„": str(img_path),
                    "é¢„æµ‹ç±»åˆ«": pred_class,
                    "çœŸå®ç±»åˆ«": true_class,
                    "é¢„æµ‹æ¦‚ç‡": {
                        "æ­£å¸¸å‘ç¥¨": round(float(student_probs[0]), 4),
                        "å¼‚å¸¸å‘ç¥¨": round(float(student_probs[1]), 4)
                    },
                    "æ¨ç†æ—¶é—´(ç§’)": infer_time,
                    "å­¦ç”Ÿæ¨¡å‹logits": student_logits.cpu().numpy()[0].tolist(),
                    "æ•™å¸ˆæ¨¡å‹logits": teacher_logits.cpu().numpy()[0].tolist()
                })

                print(f"âœ… è¯†åˆ«æˆåŠŸ: é¢„æµ‹={pred_class} | çœŸå®={true_class}")
                print(f"   æ¦‚ç‡åˆ†å¸ƒ: æ­£å¸¸={student_probs[0]:.3f}, å¼‚å¸¸={student_probs[1]:.3f}")

            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥: {str(e)}")
                import traceback
                traceback.print_exc()
                continue

    # 4. è®¡ç®—æŒ‡æ ‡
    accuracy, precision, recall, f1 = calculate_metrics(all_predictions, all_targets)

    print(f"\n===== å¤„ç†å®Œæˆ =====")
    print(f"âœ… æˆåŠŸå¤„ç† {len(recognition_results)}/1 å¼ å›¾åƒ")
    print("ğŸ“Š è¯†åˆ«æ€§èƒ½æŒ‡æ ‡ï¼š")
    print(f"   å‡†ç¡®ç‡: {accuracy * 100:.2f}%")
    print(f"   ç²¾ç¡®ç‡: {precision * 100:.2f}%")
    print(f"   å¬å›ç‡: {recall * 100:.2f}%")
    print(f"   F1åˆ†æ•°: {f1 * 100:.2f}%")

    # 5. ç”Ÿæˆå¹¶ä¿å­˜JSONç»“æœ
    final_result = {
        "æ¨¡å‹ä¿¡æ¯": {
            "æ•™å¸ˆæ¨¡å‹": "æ¨¡æ‹ŸDeepSeekè§†è§‰åˆ†ç±»æ¨¡å‹",
            "å­¦ç”Ÿæ¨¡å‹": "è½»é‡çº§CNN",
            "ä»»åŠ¡ç±»å‹": "å‘ç¥¨äºŒåˆ†ç±»ï¼ˆæ­£å¸¸/å¼‚å¸¸ï¼‰",
            "ä½¿ç”¨è®¾å¤‡": str(device)
        },
        "æ€§èƒ½æŒ‡æ ‡": {
            "æ€»å›¾åƒæ•°": 1,
            "æˆåŠŸè¯†åˆ«æ•°": len(recognition_results),
            "å‡†ç¡®ç‡(%)": round(accuracy * 100, 2),
            "ç²¾ç¡®ç‡(%)": round(precision * 100, 2),
            "å¬å›ç‡(%)": round(recall * 100, 2),
            "F1åˆ†æ•°(%)": round(f1 * 100, 2)
        },
        "å•å›¾è¯†åˆ«ç»“æœ": recognition_results,
        "ç”Ÿæˆæ—¶é—´": time.strftime("%Y-%m-%d %H:%M:%S")
    }

    # æ‰“å°JSONæ‘˜è¦
    json_str = json.dumps(final_result, ensure_ascii=False, indent=2)
    print("\n===== è¯†åˆ«ç»“æœ =====")
    print(json_str)

    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    try:
        output_dir = os.path.dirname(output_json)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
            print(f"â„¹ï¸ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")

        with open(output_json, 'w', encoding='utf-8') as f:
            f.write(json_str)

        print(f"âœ… ç»“æœå·²ä¿å­˜è‡³: {os.path.abspath(output_json)}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")
        print("â„¹ï¸ å®Œæ•´ç»“æœå·²æ‰“å°åœ¨æ§åˆ¶å°")

    return final_result


if __name__ == "__main__":
    # è¿è¡Œå•å¼ å›¾åƒè¯†åˆ«
    invoice_recognition_single_image(
        single_image_path="C:/Users/zzh/Desktop/wechat_2025-09-29_212608_948.png",
        output_json="C:/Users/zzh/Desktop/wechat_2025-09-29_212608_948_è¯†åˆ«ç»“æœ.json"
    )